\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.

\usepackage{jmlr2e}
\usepackage[a4paper, right=3cm, left=3cm, bottom=3cm, top=3cm]{geometry}
\usepackage{amsmath, mathtools}
\usepackage{booktabs}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{hyperref}

% Definitions of handy macros can go here
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\prob}{\mathbb{P}} 
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\O}{\mathcal{O}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\eps}{\varepsilon}
\newcommand{\dd}{\, \mathrm{d}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\KL}{KL}
\newcommand{\intset}[2]{\{#1, ..., #2\}}
\newcommand{\mnbs}{\nobreak\hspace{.16667em}}


% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}
% \jmlrheading{}{}{}{}{}{Bastien LE CHENADEC, SOFIANE EZZEHI and THEILO TERRISSE}

% Short headings should be running head and authors last names

\ShortHeadings{Mixture Models for Community Detection}{B. LE CHENADEC, S. EZZEHI and T. TERRISSE}
\firstpageno{1}

\begin{document}

\title{Mixture Models for Community Detection}

\author{\name Bastien Le Chenadec \email bastien.le-chenadec@eleves.enpc.fr \\
    \addr Ecole des Ponts Paristech
    \AND
    \name Sofiane Ezzehi \email sofiane.ezzehi@eleves.enpc.fr \\
    \addr Ecole des Ponts Paristech
    \AND
    \name Theïlo Terrisse \email theilo.terrisse@eleves.enpc.fr \\
    \addr Ecole des Ponts Paristech
}

\maketitle

\textcolor{red}{Quelques remarques:
    \begin{itemize}
        \item hésitez pas à utiliser Zotero pour exporter d'éventuelles références bibliographiques, histoire d'avoir un remplissage uniforme e de gagner du temps
        \item Une fois une abbréviation introduite (ex: SBM), si possible toujours utiliser l'abbréviation et ne pas ré-écrire l'expression entière
        \item Je propose de préférer le terme de "cluster" à "community".
    \end{itemize}
}

\begin{contribstatement}
    TODO
\end{contribstatement}

% \begin{keywords}
%   Mixture Models, Graph Clustering, Community Detection, EM algorithm
% \end{keywords}

\section{Introduction}

Garph clustering, also referred to as ``community detection'' \cite{fortunato_community_2010}, is the problem of revealing clusters within a graph where a ``cluster'' is often defined as a set of nodes that are more densely connected between them than with the rest of the network.
More precisely, a cluster $C$ is defined as a group of nodes with an internal density $\delta_{int}(C)$ larger than its inter-community density $\delta_{ext}(C)$, where
\begin{equation}
    \delta_{int} = \frac{\text{\# internal edges of }C}{n_C (n_C - 1) / 2} \quad \text{and} \quad \delta_{ext} = \frac{\text{\# inter-cluster edges of C}}{n_C (n - n_C)}
\end{equation}
where $n$ is the number of nodes of the graph, $n_C$ is the number of nodes in the cluster $C$ and internal (resp. inter-cluster) edges are edges between nodes of $C$ (resp. between nodes of $C$ and nodes outside of $C$). This definition is actually characteristic of homophilious clusters, while heterophilious clusters would take be such that $\delta_{int}(C) < \delta_{ext}(C)$.
Graph clustering has tremendous applications in the study of real networks such as
social [1] or biological [4] networks and can reveal properties of complex systems at mesoscopic scales.

Community detection is known to be an NP-hard problem, as testing all possible partitions of a graph has complexity $\O(B_n)$ with $B_n$ the Bell number of order $n$. This is particularly poblematic as real networks easily grow to thousands or even millions of nodes. As well documented in \cite{fortunato_community_2010}, several classes of algorithms can be identified in the literature. To name a few, hierarchical algorithms rely on similarity measures to aggregate or divise groups of nodes; spectral algorithms also rely on a similarity matrix, but operate in the space spanned by the eigenvectors its Laplacian matrix. Yet, defining a similiarty measure between nodes is not always easy. Alternatively, modularity-based approaches solve an optimization problem that maximizes modularity (defined in Section \ref{subsec:metrics}).

In particular, contrary to algorithmic methods as the ones above, model-based methods fit a mathematical model capable of explaining the observed connectivity, and from which descriptive properties of a graph may be extracted or computed. The Stochastic Block Model (SBM) introduced in \cite{snijders_estimation_1997} is a renowned example of model for graphs. In \cite{main_article}, J.-J. Daudin \textit{et al.} study this model and propose a variational Expectation-Maximization (EM) algorithm to find parameters that best fit a given graph.

In this report, we detail the method introduced in \cite{main_article} as well as variants and alternatives from the literature in Section \ref{sec:method}. In Sections \ref{sec:experiments} and \ref{sec:discussion}, we present implementation of the method and the experients performed to assess is performance and limits. Lastly, we conclude in Section \ref{sec:conclusion}.


\section{Proposed method}
    \label{sec:method}

In "A mixture model for random graphs" \cite{main_article}, the authors propose a bayesian model for graphs, and an EM approach to fit this model to a given graph.

\subsection{A mixture model for random graphs}

We will follow the same notations as in \cite{main_article}. We consider an undirected graph with $n$ nodes and no self-loops. We denote $X$ the adjacency matrix of this graph. As such $X_{ij}\in \{0,1\}$ denotes the existence of an edge between nodes $i$ and $j$, and $\forall i\in \left\{1,\dots,n\right\}, X_{ii}=0$.

We consider a mixture model that spreads the vertices in $Q$ classes, with an a priori repartition $\{\alpha_1, \dots, \alpha_Q\}$ between classes. We introduce the random variables $Z_{iq} \in \{0,1\}$ for $i\in \left\{1,\dots,n\right\}$ and $q\in \left\{1,\dots,Q\right\}$, that represent the appartenance of node $i$ to class $q$. We have the following prior distribution on $Z$:
\begin{equation}
    \forall i\in \left\{1,\dots,n\right\}, \quad \sum_{q=1}^Q Z_{iq} = 1 \quad \text{and} \quad \forall q\in \left\{1,\dots,Q\right\},\quad \mathbb{P}(Z_{iq}=1)=\alpha_q
\end{equation}

We introduce priors on the existence of edges between nodes of different classes. We denote $\pi_{ql}$ the probability of an edge between a node of class $q$ and a node of class $l$. Because the graph is undirected, we have $\pi_{ql}=\pi_{lq}$. Finally we suppose the following prior distribution on the existence of edges between nodes :
\begin{equation}
    \forall q,l\in \left\{1,\dots,Q\right\}, \quad \forall i\neq j\in \left\{1,\dots,n\right\}, \quad \mathbb{P}(X_{ij}=1|Z_{iq}=1,Z_{jl}=1)=\pi_{ql}
\end{equation}

Figure \ref{fig:graphical_model} shows the graphical model of this mixture model.

\begin{figure}[H]
    \centering
    \label{fig:graphical_model}
    \tikz{
        \node[obs] (X_{ij}) {$X_{ij}$};
        \node[latent,left=of X_{ij}, xshift=-1cm, label={[name=label1,text height=1.2em]below:$i=1,\dots,n$}] (Z_i) {$Z_i$};
        \node[latent,right=of X_{ij}, xshift=1cm, label={[name=label2,text height=1.2em]below:$j=i+1,\dots,n$}] (Z_j) {$Z_j$};
        \node[const, above=of X_{ij}](alpha){$\alpha$};
        \node[const, below=of X_{ij}, yshift=-1cm](pi){$\pi$};
        \plate [inner sep=.5cm] {} {(Z_i)(X_{ij})(Z_j)(label1)(label2)} {};
        \plate [inner sep=.25cm] {} {(Z_j)(X_{ij})(label2)} {};
        \edge {Z_i,Z_j} {X_{ij}}
        \edge {alpha} {Z_i}
        \edge {alpha} {Z_j}
        \edge {pi} {X_{ij}}
    }
    \caption{Graphical model of the mixture model}
\end{figure}


\subsection{Variational Expectation-Maximization algorithm}

The log-likelihood of the model is given by :
\begin{equation}
    \log \mathcal{L}(X, Z) = \sum_{i}\sum_{q} Z_{iq}\log\alpha_q + \frac{1}{2}\sum_{i\neq j}\sum_{q,l} Z_{iq}Z_{jl} \times \pi_{ql}^{X_{ij}}(1-\pi_{ql})^{1-X_{ij}}
\end{equation}

Because the likelihood $\mathcal{L}(X)$ is not tractable, the authors propose to use an EM algorithm to fit the model. However the E-step is not tractable either because of the posterior distribution of $Z$ given $X$. Instead the authors propose to optimize (\ref{eq:lower_bound}) which is a lower bound of $\log\mathcal{L}(X)$ obtained using the Kullback-Leibler divergence between the posterior distribution of $Z$ given $X$ and an approximated distribution $R_X$.

\begin{equation}
    \label{eq:lower_bound}
    \mathcal{J}(R_X)=\log \mathcal{L}(X)-\KL[R_X(\cdot), P(\cdot|X)]
\end{equation}

By choosing the approximated distribution $R_X$ to be a product of independant multinomial distributions (\ref{eq:approximated_distribution}), the authors obtain a fixed point relation between the parameters of the model and the parameters of the approximated distribution maximizing the lower bound $\mathcal{J}(R_X)$ (\ref{eq:fixed_point}). This fixed point relation is used in the E-step of the algorithm.

\begin{equation}
    \label{eq:approximated_distribution}
    R_X(Z)=\prod_{i=1}^n h(Z_i, \tau_i) \quad\quad \forall i \in \left\{1,\dots,n\right\}, h(Z_i, \tau_i)=\prod_{q=1}^Q \tau_{iq}^{Z_{iq}}
\end{equation}

\begin{equation}
    \label{eq:fixed_point}
    \forall i \in \left\{1,\dots,n\right\}, \forall q \in \left\{1,\dots,Q\right\}, \hat{\tau}_{iq}\propto \alpha_q \prod_{j\neq i}\prod_l \left[\pi_{ql}^{X_{ij}}(1-\pi_{ql})^{1-X_{ij}}\right]^{\hat{\tau}_{jl}}
\end{equation}

This fixed point relation doesn't assure the theoretical convergence of the algorithm. We will see later that the convergence of the algorithm is not guaranteed in practice either. Finally we have the following updates for the M-step maximizing $\mathcal{J}(R_X)$ :

\begin{equation}
    \label{eq:m_step}
    \forall q, l \in \left\{1,\dots,Q\right\}, \quad
    \hat{\alpha}_q=\frac{1}{n}\sum_{i} \hat{\tau}_{iq}
    \quad
    \hat{\pi}_{ql}=\frac{\sum_{i\neq j} \hat{\tau}_{iq}\hat{\tau}_{jl}X_{ij}}{\sum_{i\neq j} \hat{\tau}_{iq}\hat{\tau}_{jl}}
\end{equation}


\subsection{A variant}

[Sofiane: présenter la variante]


\section{Experiments}
\label{sec:experiments}

\subsection{Implementation}


\subsection{Metrics}
    \label{subsec:metrics}

\subsection{Testing on SBM datasets}

\subsubsection{Fitting a model}

\subsubsection{Initialization sensitivity}


\subsection{Comparison to variants}

\subsubsection{Datasets}

\subsubsection{Evaluation metrics}

\subsubsection{Results}


\section{Analysis and discussion}
\label{sec:discussion}

TODO



\section{Conclusion}
\label{sec:conclusion}

TODO



% Acknowledgements should go at the end, before appendices and references
\acks{Est-ce qu'on en met ?}

\newpage

\appendix
\section{plots}

 [...]

\bibliographystyle{abbrv}
\bibliography{bibliography}

\end{document}